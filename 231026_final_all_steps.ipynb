{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1fd2bee",
   "metadata": {},
   "source": [
    "# Label-Free in vivo Quantification of the Vascular Network in Murine Colitis using Transrectal Absorber Guide Raster-Scanning Optoacoustic Mesoscopy (TAG-RSOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d16ebc",
   "metadata": {},
   "source": [
    "The following code provides a pipeline to pick up results generaded by the Fiji-pipeline, calculate the parameters and to build the tabels to be analysed in Prism (statistic software)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63e93ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65624b8",
   "metadata": {},
   "source": [
    "# Choose Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66ef3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [r\"D:\\newPipeline_v4.0\\final_5_medianF_after_Seg\\4_AT\",\n",
    "                r\"D:\\newPipeline_v4.0\\final_5_medianF_after_Seg\\5_RF\",\n",
    "                r\"D:\\newPipeline_v4.0\\final_5_medianF_after_Seg\\6_VF\"]\n",
    "path_analysis = \"D:/newPipeline_v4.0/final_5_medianF_after_Seg/Auswertung/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123966d",
   "metadata": {},
   "source": [
    "# Normalized Number of Connected Components\n",
    "The number of networks with two or more connected branches (vessels) in an image is defined as the number of components that have one or more junctions. This information is calculated by Fiji's analyse skeleton function, called in the macro \"skeletonize_analyse_and_save_Results.ijm\" and saved as file ending with \"_results.csv\".\n",
    "\n",
    "    ~~Connected Components (CC) is natural number has no unit~~\n",
    "\n",
    "To normalize the CC it is devided by the area of the used colon 2D image in cm². The colon area is saved as number of pixels in a file ending with \"\\_vesselArea.csv\" when calling any segmentation macro.\n",
    "\n",
    "(AREA_COLON_pixel * 20µm * 20µm) / 100000000 = AREA_COLON_cm²\n",
    "\n",
    "    Normalized Number of Connected Components (CC_n) has the unit (cm^-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7be9a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NNCC(f):\n",
    "    #df_CC =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    df_NNCC =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        df_skeleton = pd.read_csv(os.path.join(f, skeleton_name))\n",
    "        df_area = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_vesselArea.csv\")))\n",
    "        AREA_COLON_pixel = df_area.iloc[0][\"Area\"] # Returns number of pixels\n",
    "        AREA_COLON_CM2 = AREA_COLON_pixel * 0.000004\n",
    "        \n",
    "        number_CC = 0\n",
    "        for index, row in df_skeleton.iterrows(): # Iterate throu components (networks)\n",
    "            if(row['# Junctions'] != 0): # Def. Connected Components\n",
    "                number_CC += 1\n",
    "        \n",
    "        #df_CC.loc[len(df_CC)] = [skeleton_name.replace(\"_results.csv\", \"\"), number_CC]\n",
    "        df_NNCC.loc[len(df_NNCC)] = [skeleton_name.replace(\"_results.csv\", \"\"), number_CC / AREA_COLON_CM2]\n",
    "    \n",
    "    \n",
    "    #df_CC.to_csv(f+\"/Results\"+\"/Connected Components.csv\", header=False, index=False)\n",
    "    df_NNCC.to_csv(f+\"/Results\"+\"/Normalized Number of Connected Components.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a5b94",
   "metadata": {},
   "source": [
    "# Normalized Number of Branches\n",
    "The number of branches (vessels) in a skeleton is calculated by Fiji's analyse skeleton function, called in the macro \"skeletonize_analyse_and_save_Results.ijm\" and saved as file ending with \"_BranchInfo.csv\".\n",
    "\n",
    "To normalize the NB it is devided by the area of the used colon 2D image in cm². The colon area is saved as number of pixels in a file ending with \"\\_vesselArea.csv\" when calling any segmentation macro.\n",
    "\n",
    "(AREA_COLON_pixel * 20µm * 20µm) / 100000000 = AREA_COLON_cm²\n",
    "\n",
    "    Normalized Number of Branches (NNB) has the unit (cm^-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0033c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NNB(f):\n",
    "    df_NNB =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        df_area = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_vesselArea.csv\")))\n",
    "        AREA_COLON_pixel = df_area.iloc[0][\"Area\"] # Returns number of pixels\n",
    "        AREA_COLON_CM2 = AREA_COLON_pixel * 0.000004\n",
    "        df_branches = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_BranchInfo.csv\")))\n",
    "        \n",
    "        df_NNB.loc[len(df_NNB)] = [skeleton_name.replace(\"_results.csv\", \"\"), df_branches.shape[0] / AREA_COLON_CM2]\n",
    "        \n",
    "        \n",
    "    df_NNB.to_csv(f+\"/Results\"+\"/Normalized Number of Branches.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec215a",
   "metadata": {},
   "source": [
    "# Number of Branches in Largest Component\n",
    "The number of branches (vessels) in a skeleton is calculated by Fiji's analyse skeleton function, called in the macro \"skeletonize_analyse_and_save_Results.ijm\" and saved as file ending with \"_BranchInfo.csv\".\n",
    "\n",
    "    The Number of Branches in Largest Component (LC_NB) has no unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81894b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_LC_NB(f):\n",
    "    LC_NB =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        df_branches = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_BranchInfo.csv\")))\n",
    "        df_skeleton = pd.read_csv(os.path.join(f, skeleton_name))\n",
    "        \n",
    "        number_CC = 0\n",
    "        index_LC = 0\n",
    "        for index, row in df_skeleton.iterrows(): # Iterate throu components (networks)\n",
    "            if(row['# Junctions'] != 0): # Def. Connected Components\n",
    "                number_CC += 1\n",
    "                index_LC = index\n",
    "        \n",
    "        df_branches_MC = df_branches[df_branches['Skeleton ID'] == index_LC+1]\n",
    "        LC_NB.loc[len(LC_NB)] = [skeleton_name.replace(\"_results.csv\", \"\"), df_branches_MC.shape[0]]\n",
    "        \n",
    "        \n",
    "    LC_NB.to_csv(f+\"/Results\"+\"/Largest Component_Number of Branches.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc13081",
   "metadata": {},
   "source": [
    "# Length of Largest Component\n",
    "The largest component is calculated as the number of pixels representing junction-, end-, or slab-pixels. This information is calculated by Fiji's analyse skeleton function, called in the macro \"skeletonize_analyse_and_save_Results.ijm\" and saved as file ending with \"_results.csv\".\n",
    "\n",
    "    Largest Component (CC) is a length and has the unit (mm)\n",
    "    \n",
    "Calculated by number of pixels in LC * 20 µm / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "498c295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_LC_L(f):\n",
    "    df_LC_L =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        df_skeleton = pd.read_csv(os.path.join(f, skeleton_name))\n",
    "        \n",
    "        tmp_LC = 0\n",
    "        for index, row in df_skeleton.iterrows(): # Iterate throu components (networks)\n",
    "            if(row['# End-point voxels'] + row['# Junction voxels'] + row['# Slab voxels'] > tmp_LC):\n",
    "                tmp_LC = row['# End-point voxels'] + row['# Junction voxels'] + row['# Slab voxels']\n",
    "                \n",
    "        df_LC_L.loc[len(df_LC_L)] = [skeleton_name.replace(\"_results.csv\", \"\"), tmp_LC * 0.02]\n",
    "    \n",
    "    \n",
    "    df_LC_L.to_csv(f+\"/Results\"+\"/Largest Components_Total Length.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1bdbb",
   "metadata": {},
   "source": [
    "# Normalized Network Length\n",
    "The Normalized Network Length is calculated by deviding the total number of junction-, end-, or slab-pixels by the colon area. This information is calculated by Fiji's analyse skeleton function, called in the macro \"skeletonize_analyse_and_save_Results.ijm\" and saved as file ending with \"_results.csv\". The colon area is saved as number of pixels in a file ending with \"\\_vesselArea.csv\" when calling any segmentation macro.\n",
    "\n",
    "    Normalized Network Length (NNL) has the unit (µm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a746ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NNL(f):\n",
    "    df_NNL =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        df_skeleton = pd.read_csv(os.path.join(f, skeleton_name))\n",
    "        df_area = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_vesselArea.csv\")))\n",
    "        AREA_COLON_pixel = df_area.iloc[0][\"Area\"] # Returns number of pixels\n",
    "        \n",
    "        number_pixels = 0\n",
    "        for index, row in df_skeleton.iterrows(): # Iterate throu components (networks)\n",
    "            number_pixels += row['# End-point voxels'] + row['# Junction voxels'] + row['# Slab voxels']\n",
    "        \n",
    "        df_NNL.loc[len(df_NNL)] = [skeleton_name.replace(\"_results.csv\", \"\"), number_pixels/(AREA_COLON_pixel*20)]\n",
    "    \n",
    "    \n",
    "    df_NNL.to_csv(f+\"/Results\"+\"/Normalized Network Length.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426eb006",
   "metadata": {},
   "source": [
    "# Normalized Vessel Area\n",
    "the normalized vessel area is calculated by the percentage of non-zero pixels in the colon area. This information is saved as % in a file ending with \"\\_vesselArea.csv\" when calling any segmentation macro.\n",
    "\n",
    "    Normalized Vessel Area (NVA) has the unit (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51670011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NVA(f):\n",
    "    df_NVA =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        df_area = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_vesselArea.csv\")))\n",
    "        \n",
    "        df_NVA.loc[len(df_NVA)] = [skeleton_name.replace(\"_results.csv\", \"\"), df_area.iloc[0][\"%Area\"]]\n",
    "    \n",
    "    \n",
    "    df_NVA.to_csv(f+\"/Results\"+\"/Normalized Vessel Area.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e24bb",
   "metadata": {},
   "source": [
    "# Average Vessel Diameter\n",
    "The average vessel diameter is calculated by deviding the total vessel area by the total vessel length.\n",
    "\n",
    "    Average Vessel Diameter (AVD) has the unit (µm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd56b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_AVD(f):\n",
    "    df_AVD =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        df_skeleton = pd.read_csv(os.path.join(f, skeleton_name))\n",
    "        df_area = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_vesselArea.csv\")))\n",
    "        AREA_VESSEL_pixels = (df_area.iloc[0][\"Area\"] / 100) * df_area.iloc[0][\"%Area\"] # Returns number of pixels\n",
    "        \n",
    "        number_pixels = 0\n",
    "        for index, row in df_skeleton.iterrows(): # Iterate throu components (networks)\n",
    "            number_pixels += row['# End-point voxels'] + row['# Junction voxels'] + row['# Slab voxels']\n",
    "        \n",
    "        df_AVD.loc[len(df_AVD)] = [skeleton_name.replace(\"_results.csv\", \"\"), (AREA_VESSEL_pixels*20)/number_pixels]\n",
    "    \n",
    "    \n",
    "    df_AVD.to_csv(f+\"/Results\"+\"/Average Vessel Diameter.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c4367",
   "metadata": {},
   "source": [
    "# Average Vessel Diameter (Distance Map)\n",
    "The average vessel diameter is calculated based on the histogram of the geographical distance map.\n",
    "\n",
    "    Average Vessel Diameter (Distance Map) (AVD_DM) has the unit (µm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c4b6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_AVD_DM(f):\n",
    "    df_AVD_DM =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        AVD_DM = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_vesselThickness.csv\")))\n",
    "        # Drop first and zero lines\n",
    "        AVD_DM = AVD_DM.drop(0)\n",
    "        AVD_DM = AVD_DM[AVD_DM['Count'] != 0]\n",
    "        AVD_DM_mm = (AVD_DM['Distance'] * AVD_DM['Count']*2*20).sum() / AVD_DM['Count'].sum()\n",
    "        \n",
    "        df_AVD_DM.loc[len(df_AVD_DM)] = [skeleton_name.replace(\"_results.csv\", \"\"), AVD_DM_mm]\n",
    "    \n",
    "    \n",
    "    df_AVD_DM.to_csv(f+\"/Results\"+\"/Average Vessel Diameter_Distance Map.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66fccd",
   "metadata": {},
   "source": [
    "# Normalized Blood Volume (Distance Map)\n",
    "The normalized Blood Volume is calculated based on the geographic distance map relative to the colon area imaged.\n",
    "\n",
    "    Normalized Blood Volume (Distance Map) (NBV_DM) has the unit (µm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbebf15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NBV_DM(f):\n",
    "    df_NBV_DM =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        # Open nessesary files\n",
    "        df_area = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_vesselArea.csv\")))\n",
    "        AREA_COLON_pixel = df_area.iloc[0][\"Area\"] # Returns number of pixels\n",
    "        AVD_DM = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_vesselThickness.csv\")))\n",
    "        # Drop first and zero lines\n",
    "        AVD_DM = AVD_DM.drop(0)\n",
    "        AVD_DM = AVD_DM[AVD_DM['Count'] != 0]\n",
    "        AVD_DM_mm = ((AVD_DM['Distance']*20)**2 * 3.1416 *AVD_DM['Count']*20).sum() / (AREA_COLON_pixel*20*20)\n",
    "        \n",
    "        df_NBV_DM.loc[len(df_NBV_DM)] = [skeleton_name.replace(\"_results.csv\", \"\"), AVD_DM_mm]\n",
    "    \n",
    "    \n",
    "    df_NBV_DM.to_csv(f+\"/Results\"+\"/Normalized Blood Volume_Distance Map.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f6dfd",
   "metadata": {},
   "source": [
    "# Perimeter-Area Ratio\n",
    "The perimeter-area ratio is calculated as the mean value of the ratios of length and area anclosed by the component (conhex hull)\n",
    "\n",
    "    Perimeter Area Ratio (PAR) has the unit (µm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a1406aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PAR(f):\n",
    "    df_PAR =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        df_skeleton = pd.read_csv(os.path.join(f, skeleton_name))\n",
    "        df_convex_hulls = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_convex_hulls.csv\")))\n",
    "        df = pd.concat([df_skeleton, df_convex_hulls], axis=1)\n",
    "        df_peri_area_ratio = (((df[\"# End-point voxels\"] + df[\"# Junction voxels\"] + df[\"# Slab voxels\"])*20) / df[\"Area\"]).mean()\n",
    "        df_PAR.loc[len(df_PAR)] = [skeleton_name.replace(\"_results.csv\", \"\"), df_peri_area_ratio]\n",
    "    \n",
    "    \n",
    "    df_PAR.to_csv(f+\"/Results\"+\"/Perimeter Area Ratio.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c2a03",
   "metadata": {},
   "source": [
    "# Fractal Dimension\n",
    "The fractal dimension is calculated by the box-counting method (using Fiji)\n",
    "\n",
    "    Fractal Dimension (FD) has no unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b4385ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_FD(f):\n",
    "    df_FD =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        df_fd = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_box_count.csv\")))\n",
    "        df_FD.loc[len(df_FD)] = [skeleton_name.replace(\"_results.csv\", \"\"), df_fd[\"D\"].mean()] # Calling .mean() is a dirty workaround to avoid getting dtaype and row number\n",
    "    \n",
    "    \n",
    "    df_FD.to_csv(f+\"/Results\"+\"/Fractal Dimension.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e036be",
   "metadata": {},
   "source": [
    "# Cord-to-length ratio\n",
    "The cord-to-length ratio (CLR) is the ratio between the Eukledian distance of the ends and the length of a vessel. A straight vessel has a CLR of 1. Circles have to be pruned to prevent infinite values.\n",
    "\n",
    "    Cord to Length Ratio (CLR) has not unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c8e78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_CLR(f):\n",
    "    df_CLR =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        df_branches = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_BranchInfo.csv\")))\n",
    "        clr = []\n",
    "        for index, row in df_branches.iterrows():\n",
    "            if row['Branch length'] > 1 and row['Euclidean distance'] > 1:\n",
    "                clr.append(row['Branch length'] / row['Euclidean distance'])\n",
    "        df_CLR.loc[len(df_CLR)] = [skeleton_name.replace(\"_results.csv\", \"\"), sum(clr)/len(clr)]\n",
    "    \n",
    "    \n",
    "    df_CLR.to_csv(f+\"/Results\"+\"/Cord to Length Ratio.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193ead6",
   "metadata": {},
   "source": [
    "# Cord-to-length ratio in largest component\n",
    "See cord-to-length ratio\n",
    "\n",
    "    Cord to Length Ratio in Largest Component (CLR) has not unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74005f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_LC_CLR(f):\n",
    "    df_LC_CLR =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for skeleton_name in sorted([file for file in os.listdir(f) if file.endswith(\"_results.csv\")]): # Iterate throu individuals\n",
    "        df_skeleton = pd.read_csv(os.path.join(f, skeleton_name))\n",
    "        df_branches = pd.read_csv(os.path.join(f, skeleton_name.replace(\"_results.csv\", \"_BranchInfo.csv\")))\n",
    "        \n",
    "        LC = 0\n",
    "        LC_index = 0\n",
    "        for index, row in df_skeleton.iterrows():\n",
    "            if(row['# End-point voxels'] + row['# Junction voxels'] + row['# Slab voxels'] > LC):\n",
    "                LC = row['# End-point voxels'] + row['# Junction voxels'] + row['# Slab voxels']\n",
    "                LC_index = index\n",
    "        \n",
    "        clr = []\n",
    "        for index, row in df_branches[df_branches['Skeleton ID'] == LC_index+1].iterrows():\n",
    "            if row['Branch length'] > 1 and row['Euclidean distance'] > 1:\n",
    "                clr.append(row['Branch length'] / row['Euclidean distance'])\n",
    "        \n",
    "        df_LC_CLR.loc[len(df_LC_CLR)] = [skeleton_name.replace(\"_results.csv\", \"\"), sum(clr)/len(clr)]\n",
    "    \n",
    "    \n",
    "    df_LC_CLR.to_csv(f+\"/Results\"+\"/Largest Component_Cord to Length Ratio.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36538036",
   "metadata": {},
   "source": [
    "# Calculate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5c50877",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in folder_paths:\n",
    "    output_path = os.path.join(f, \"Results\")\n",
    "    if not os.path.exists(output_path):\n",
    "        try: os.makedirs(output_path)\n",
    "        except OSError as e: print(f\"Error creating folder '{output_path}': {e}\")\n",
    "    \n",
    "    calculate_NNCC(f) # Normalized Number of Connected Components\n",
    "    calculate_NNB(f) # Normalized Number of Branches\n",
    "    calculate_LC_NB(f) # Number of Branches in Largest Component\n",
    "    calculate_LC_L(f) # Length of Largest Components\n",
    "    calculate_NNL(f) # Normalized Network Length\n",
    "    calculate_NVA(f) # Normalized Vessel Area\n",
    "    calculate_AVD(f) # Average Vessel Diameter\n",
    "    calculate_AVD_DM(f) # Average Vessel Diameter_Distance Map\n",
    "    calculate_NBV_DM(f) # Normalized Blood Volume_Distance Map\n",
    "    calculate_PAR(f) # Perimeter Area Ratio\n",
    "    calculate_FD(f) # Fractal Dimension\n",
    "    calculate_CLR(f) # Cord to Length Ratio\n",
    "    calculate_LC_CLR(f) # Cord to Length Ratio in Largest Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab703dc",
   "metadata": {},
   "source": [
    "# Merge data tables into one master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42adef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame()\n",
    "\n",
    "for p in folder_paths:\n",
    "    path = p + \"/Results\"\n",
    "    files = os.listdir(path)\n",
    "    # create a list of DataFrames from .csv files\n",
    "    dfs = []\n",
    "    for i in files:\n",
    "        if i.endswith(\".csv\"): # exclude non-.csv\n",
    "            #print(path + \"/\" + i)\n",
    "            tmp = pd.read_csv(path + \"/\" + i, header=None)\n",
    "            tmp.columns = ['name', i.replace(\".csv\", \"\")] # rename colums in 'name' and 'paramerer'-> filename\n",
    "            dfs.append(tmp)\n",
    "            \n",
    "    df = pd.concat(dfs, axis=1) # merge all frames into one\n",
    "    df = df.loc[:, ~df.columns.duplicated()] # drop duplicates\n",
    "    \n",
    "    #Split the column 'name' into single variables\n",
    "    ########################################################\n",
    "    \n",
    "    # Split the first column using \"_\" as delimiter\n",
    "    split_df = df['name'].str.split('_', expand=True)\n",
    "    # Rename the columns in the split DataFrame\n",
    "    split_df.columns = ['study', 'ID', 'Day', 'contrast']\n",
    "    \n",
    "    # concat new columns and drop \"name\"\n",
    "    concat_df = pd.concat([split_df, df.drop(columns=['name'])], axis=1)\n",
    "    \n",
    "    if p.endswith(\"VF\"):\n",
    "        concat_df.insert(4, 'Method', \"VF\")\n",
    "    elif p.endswith(\"RF\"):\n",
    "        concat_df.insert(4, 'Method', \"RF\")\n",
    "    elif p.endswith(\"AT\"):\n",
    "        concat_df.insert(4, 'Method', \"AT\")\n",
    "    else:\n",
    "        print(\"An error occurred! Worng Folder name?\")\n",
    "        \n",
    "    all_df = pd.concat([all_df, concat_df], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "all_df.to_excel(path_analysis + \"all.xlsx\", index=False)\n",
    "#all_df.to_csv(path_analysis + \"all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db02271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66cc8547",
   "metadata": {},
   "source": [
    "# Export tables for statistic program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6d6160b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(path_analysis + \"all.xlsx\")\n",
    "m = \"VF\"\n",
    "\n",
    "collectives = [\n",
    "            [\n",
    "                \"Method\",\n",
    "                [\n",
    "                    ['AT', 'hc2', 0, \"AT\", \"w\"],\n",
    "                    ['VF', 'hc2', 0, \"VF\", \"w\"],\n",
    "                    ['RF', 'hc2', 0, \"RF\", \"w\"]\n",
    "                ]\n",
    "            ],\n",
    "            [\n",
    "                \"1xDSS\",\n",
    "                [\n",
    "                    ['HC Day 0', 'hc2', 0, m, \"w\"],\n",
    "                    ['HC Day 7', 'hc2', 9, m, \"w\"],\n",
    "                    ['1xDSS Day 0', 'a3d', 0, m, \"w\"],\n",
    "                    ['1xDSS Day 9', 'a3d', 9, m, \"w\"]\n",
    "                ]\n",
    "            ],\n",
    "            [\n",
    "                \"2xDSS\",\n",
    "                [\n",
    "                    ['2xDSS Day 0', 'l2d', 0, m, \"w\"],\n",
    "                    ['2xDSS Day 7', 'l2d', 7, m, \"w\"],\n",
    "                    ['2xDSS Day 21', 'l2d', 21, m, \"w\"],\n",
    "                    ['2xDSS Day 28', 'l2d', 28, m, \"w\"]\n",
    "                ]\n",
    "            ]\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac95a3c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(by=[\"study\", 'Day', 'ID'])\n",
    "\n",
    "for parameter in df.columns[5:]:\n",
    "    for c in collectives:\n",
    "        columns = []\n",
    "        \n",
    "        for s in c[1]:\n",
    "            df_Method = df_sorted[df_sorted['Method'] == s[3]] \n",
    "            df_Method_Contrast = df_Method[df_Method['contrast'] == s[4]]\n",
    "            df_M_C = df_Method_Contrast.drop('Method', axis=1)\n",
    "            df_M_C = df_M_C.drop('contrast', axis=1)\n",
    "            parameters_to_keep = [\"study\", \"ID\", \"Day\", parameter]\n",
    "            tmp_df = df_M_C[parameters_to_keep]\n",
    "            \n",
    "            column = tmp_df[(tmp_df['study'] == s[1]) & (tmp_df['Day'] == s[2])][parameter] # for validation incert comment -> #[parameter]\n",
    "            column.reset_index(drop=True, inplace=True)\n",
    "            column.name = s[0] + \" (n = \" + str(len(column)) + \")\"\n",
    "            columns.append(column)\n",
    "            \n",
    "        pd.concat(columns, axis=1).to_excel(path_analysis + \"/\" + c[0] + \"_\" + parameter + \".xlsx\", index=False)\n",
    "        pd.concat(columns, axis=1).to_csv(path_analysis + \"/\" + c[0] + \"_\" + parameter + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3f692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c27406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d217bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d454a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6aec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28c516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546681a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b5b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c29576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fee2223",
   "metadata": {},
   "source": [
    "# Test Microvessels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc3c9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [r\"D:\\newPipeline_v4.0\\test_microvasculatur\\4_AT\",\n",
    "                r\"D:\\newPipeline_v4.0\\test_microvasculatur\\5_1_RF\",\n",
    "                r\"D:\\newPipeline_v4.0\\test_microvasculatur\\7_test_all\",\n",
    "                r\"D:\\newPipeline_v4.0\\test_microvasculatur\\6_VF\"]\n",
    "path_analysis = \"D:/newPipeline_v4.0/test_microvasculatur/Auswertung/\"\n",
    "\n",
    "\n",
    "def calculated_MV(f):\n",
    "    df_MV =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for name in sorted([file for file in os.listdir(f) if file.endswith(\"_avg_intensity_betweenvessels.csv\")]): # Iterate throu individuals\n",
    "        df_histo = pd.read_csv(os.path.join(f, name))\n",
    "        MV = df_histo.drop(0)\n",
    "        mean_MV = (MV['Value']*MV['Count']).sum() / MV['Value'].sum()\n",
    "        \n",
    "        df_MV.loc[len(df_MV)] = [name.replace(\"_avg_intensity_betweenvessels.csv\", \"\"), mean_MV]\n",
    "    \n",
    "    \n",
    "    df_MV.to_csv(f+\"/Results\"+\"/Micro Vessels.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb0bdd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in folder_paths:\n",
    "    output_path = os.path.join(f, \"Results\")\n",
    "    if not os.path.exists(output_path):\n",
    "        try: os.makedirs(output_path)\n",
    "        except OSError as e: print(f\"Error creating folder '{output_path}': {e}\")\n",
    "    calculated_MV(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8dcfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d96ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249dcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8c5974",
   "metadata": {},
   "source": [
    "# Tag depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd6f5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [r\"D:\\newPipeline_v4.0\\test_depth\\rLabs_export\"]\n",
    "path_analysis = \"D:/newPipeline_v4.0/test_depth/Auswertung/\"\n",
    "\n",
    "\n",
    "def calculated_Depth(f):\n",
    "    df_depth =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for name in sorted([file for file in os.listdir(f) if file.endswith(\".csv\")]): # Iterate throu individuals\n",
    "        data_file = pd.read_csv(os.path.join(f, name))\n",
    "        # ,Area,X,Y,Slice\n",
    "        mean_depth = ((4*data_file['Y'].sum()) / (data_file['Y'].shape[0])) / 1000 # *4 -> µm    # / 100 -> cm\n",
    "        df_depth.loc[len(df_depth)] = [name.replace(\"_segmentSize_50.csv\", \"\"), mean_depth]\n",
    "    \n",
    "    \n",
    "    df_depth.to_csv(f+\"/Results\"+\"/Depth_in_cm.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e71f388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in folder_paths:\n",
    "    output_path = os.path.join(f, \"Results\")\n",
    "    if not os.path.exists(output_path):\n",
    "        try: os.makedirs(output_path)\n",
    "        except OSError as e: print(f\"Error creating folder '{output_path}': {e}\")\n",
    "    calculated_Depth(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame()\n",
    "\n",
    "for p in folder_paths:\n",
    "    path = p + \"/Results\"\n",
    "    files = os.listdir(path)\n",
    "    # create a list of DataFrames from .csv files\n",
    "    dfs = []\n",
    "    for i in files:\n",
    "        if i.endswith(\".csv\"): # exclude non-.csv\n",
    "            #print(path + \"/\" + i)\n",
    "            tmp = pd.read_csv(path + \"/\" + i, header=None)\n",
    "            tmp.columns = ['name', i.replace(\".csv\", \"\")] # rename colums in 'name' and 'paramerer'-> filename\n",
    "            dfs.append(tmp)\n",
    "            \n",
    "    df = pd.concat(dfs, axis=1) # merge all frames into one\n",
    "    df = df.loc[:, ~df.columns.duplicated()] # drop duplicates\n",
    "    \n",
    "    #Split the column 'name' into single variables\n",
    "    ########################################################\n",
    "    \n",
    "    # Split the first column using \"_\" as delimiter\n",
    "    split_df = df['name'].str.split('_', expand=True)\n",
    "    # Rename the columns in the split DataFrame\n",
    "    split_df.columns = ['study', 'ID', 'Day', 'contrast']\n",
    "    \n",
    "    # concat new columns and drop \"name\"\n",
    "    concat_df = pd.concat([split_df, df.drop(columns=['name'])], axis=1)\n",
    "        \n",
    "    all_df = pd.concat([all_df, concat_df], ignore_index=True)\n",
    "    \n",
    "all_df.to_excel(path_analysis + \"depth.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe68673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c6cc25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dfdfcd0",
   "metadata": {},
   "source": [
    "# Tag angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c7a9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "folder_paths = [r\"D:\\newPipeline_v4.0\\test_depth\\rLabs_export\"]\n",
    "path_analysis = \"D:/newPipeline_v4.0/test_depth/Auswertung/\"\n",
    "\n",
    "\n",
    "def calculated_Angle(f):\n",
    "    df_angle =  pd.DataFrame(columns=['Name', 'Value'])\n",
    "    \n",
    "    \n",
    "    for name in sorted([file for file in os.listdir(f) if file.endswith(\".csv\")]): # Iterate throu individuals\n",
    "        data_file = pd.read_csv(os.path.join(f, name))\n",
    "        sign = 1\n",
    "        b = 50*len(data_file) / 2\n",
    "        a = data_file['Y'][1] - data_file['Y'][len(data_file)-1] # second value alway TAG coordinate\n",
    "        if a < 0:\n",
    "            sign = -sign\n",
    "            a = -a\n",
    "        \n",
    "        \n",
    "        df_angle.loc[len(df_angle)] = [name.replace(\"_segmentSize_50.csv\", \"\"), sign * math.degrees(math.tan((4*a)/(20*b)))]\n",
    "    \n",
    "    \n",
    "    df_angle.to_csv(f+\"/Results\"+\"/Angle_in_Degree.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3511f27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n",
      "593\n",
      "492\n",
      "482\n",
      "426\n",
      "409\n",
      "411\n",
      "457\n",
      "704\n",
      "700\n",
      "488\n",
      "508\n",
      "521\n",
      "510\n",
      "570\n",
      "638\n",
      "564\n",
      "518\n",
      "542\n",
      "555\n",
      "502\n",
      "458\n",
      "614\n",
      "629\n",
      "460\n",
      "428\n",
      "549\n",
      "560\n",
      "752\n",
      "734\n",
      "569\n",
      "544\n",
      "523\n",
      "495\n",
      "533\n",
      "556\n",
      "505\n",
      "478\n",
      "570\n",
      "582\n",
      "622\n",
      "619\n",
      "579\n",
      "594\n",
      "470\n",
      "447\n",
      "463\n",
      "495\n",
      "562\n",
      "517\n",
      "439\n",
      "451\n",
      "564\n",
      "503\n",
      "412\n",
      "442\n",
      "615\n",
      "590\n",
      "520\n",
      "534\n",
      "489\n",
      "489\n",
      "574\n",
      "575\n",
      "511\n",
      "502\n",
      "514\n",
      "536\n",
      "808\n",
      "808\n",
      "510\n",
      "505\n",
      "614\n",
      "600\n",
      "504\n",
      "519\n",
      "678\n",
      "672\n",
      "642\n",
      "643\n",
      "440\n",
      "423\n",
      "620\n",
      "590\n",
      "439\n",
      "421\n",
      "496\n",
      "498\n",
      "565\n",
      "534\n",
      "525\n",
      "538\n",
      "442\n",
      "436\n",
      "633\n",
      "597\n",
      "589\n",
      "550\n",
      "459\n",
      "469\n",
      "451\n",
      "420\n",
      "453\n",
      "478\n",
      "628\n",
      "542\n",
      "490\n",
      "520\n",
      "539\n",
      "520\n",
      "521\n",
      "535\n",
      "435\n",
      "466\n",
      "545\n",
      "570\n",
      "570\n",
      "560\n",
      "508\n",
      "507\n",
      "563\n",
      "598\n",
      "499\n",
      "521\n",
      "398\n",
      "422\n",
      "409\n",
      "385\n",
      "501\n",
      "522\n",
      "477\n",
      "496\n",
      "580\n",
      "562\n",
      "455\n",
      "447\n",
      "538\n",
      "524\n"
     ]
    }
   ],
   "source": [
    "for f in folder_paths:\n",
    "    output_path = os.path.join(f, \"Results\")\n",
    "    if not os.path.exists(output_path):\n",
    "        try: os.makedirs(output_path)\n",
    "        except OSError as e: print(f\"Error creating folder '{output_path}': {e}\")\n",
    "    calculated_Angle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57f20824",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame()\n",
    "\n",
    "for p in folder_paths:\n",
    "    path = p + \"/Results\"\n",
    "    files = os.listdir(path)\n",
    "    # create a list of DataFrames from .csv files\n",
    "    dfs = []\n",
    "    for i in files:\n",
    "        if i.endswith(\".csv\"): # exclude non-.csv\n",
    "            #print(path + \"/\" + i)\n",
    "            tmp = pd.read_csv(path + \"/\" + i, header=None)\n",
    "            tmp.columns = ['name', i.replace(\".csv\", \"\")] # rename colums in 'name' and 'paramerer'-> filename\n",
    "            dfs.append(tmp)\n",
    "            \n",
    "    df = pd.concat(dfs, axis=1) # merge all frames into one\n",
    "    df = df.loc[:, ~df.columns.duplicated()] # drop duplicates\n",
    "    \n",
    "    #Split the column 'name' into single variables\n",
    "    ########################################################\n",
    "    \n",
    "    # Split the first column using \"_\" as delimiter\n",
    "    split_df = df['name'].str.split('_', expand=True)\n",
    "    # Rename the columns in the split DataFrame\n",
    "    split_df.columns = ['study', 'ID', 'Day', 'contrast']\n",
    "    \n",
    "    # concat new columns and drop \"name\"\n",
    "    concat_df = pd.concat([split_df, df.drop(columns=['name'])], axis=1)\n",
    "        \n",
    "    all_df = pd.concat([all_df, concat_df], ignore_index=True)\n",
    "    \n",
    "all_df.to_excel(path_analysis + \"angle.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f4c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
